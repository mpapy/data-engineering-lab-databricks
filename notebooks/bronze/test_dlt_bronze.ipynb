{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc80e5b-b404-438f-a728-4005c546e455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "# Parametry\n",
    "input_path = \"/Volumes/principal_lab_db/landing/operational_data/agents/\"\n",
    "target_table = \"principal_lab_db.dev_bronze.agents\"\n",
    "\n",
    "# Načti všechna data (rekurzivně ze všech podadresářů)\n",
    "df = spark.read.option(\"header\", True).csv(f\"{input_path}*/*/*\")\n",
    "df = df.withColumn(\"ingestion_ts\", current_timestamp()) \\\n",
    "       .withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "# Zapiš jako Delta tabulku\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab461ea-4280-476c-8198-8f7a37875d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM principal_lab_db.dev_bronze.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "028c3f38-0502-4edd-a5e3-db5208d1478d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name, regexp_extract, to_date\n",
    "\n",
    "# --- Configuration: raw data path and catalog/schema/table names ---\n",
    "# Use the DBFS-mounted path prefix '/Volumes/...' rather than 'dbfs:/Volumes'.\n",
    "RAW_PATH = \"/Volumes/principal_lab_db/landing/operational_data\"\n",
    "CATALOG = \"principal_lab_db\"\n",
    "SCHEMA = \"dev_bronze\"\n",
    "TABLE = \"agents\"\n",
    "\n",
    "target_table = f\"{CATALOG}.{SCHEMA}.{TABLE}\"\n",
    "\n",
    "# Read all CSV files in dated subfolders: year/month/day (*.csv)\n",
    "# Pattern: three nested wildcard levels before the file\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "         .option(\"header\", True)\n",
    "         .load(f\"{RAW_PATH}/{TABLE}/*/*/*/*.csv\")\n",
    ")\n",
    "\n",
    "# Enrich with audit columns and snapshot date\n",
    "df_enriched = (\n",
    "    df.withColumn(\"ingestion_ts\", current_timestamp())\n",
    "      .withColumn(\"source_file\", input_file_name())\n",
    "      .withColumn(\n",
    "          \"snapshot_date\",\n",
    "          to_date(\n",
    "              regexp_extract(\n",
    "                  input_file_name(),\n",
    "                  r\"/(\\d{4}/\\d{2}/\\d{2})/\", 1\n",
    "              ),\n",
    "              \"yyyy/MM/dd\"\n",
    "          )\n",
    "      )\n",
    ")\n",
    "\n",
    "# Show a few rows to verify correct ingestion\n",
    "df_enriched.show(5, truncate=False)\n",
    "\n",
    "\n",
    "# Notebook 2: Write Agents to Bronze Table without DLT\n",
    "\n",
    "# Overwrite the Bronze table in Unity Catalog\n",
    "# Make sure schema has been created and managed location is configured\n",
    "\n",
    "# Use the same target_table identifier from above\n",
    "\n",
    "df_enriched.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(target_table)\n",
    "\n",
    "print(f\"✅ Data written to table '{target_table}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c60c57b3-484c-46cc-a325-05f83ad94115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name, regexp_extract, to_date\n",
    "\n",
    "# --- Konfigurace ---\n",
    "RAW_PATH     = \"/Volumes/principal_lab_db/landing/operational_data\"\n",
    "CATALOG      = \"principal_lab_db\"\n",
    "SCHEMA       = \"dev_bronze\"\n",
    "TABLE        = \"customers\"\n",
    "TARGET_TABLE = f\"{CATALOG}.{SCHEMA}.{TABLE}\"\n",
    "\n",
    "# Načti CSV ze všech podsložek year/month/day\n",
    "df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .csv(f\"{RAW_PATH}/{TABLE}/*/*/*/*.csv\")   # TADY jsou TŘI hvězdičky pro YYYY/MM/DD\n",
    ")\n",
    "\n",
    "# Obohať o audit a snapshot_date\n",
    "df_enriched = (\n",
    "    df.withColumn(\"ingestion_ts\", current_timestamp())\n",
    "      .withColumn(\"source_file\", input_file_name())\n",
    "      .withColumn(\n",
    "          \"snapshot_date\",\n",
    "          to_date(\n",
    "              regexp_extract(\n",
    "                  input_file_name(),\n",
    "                  r\"/(\\d{4}/\\d{2}/\\d{2})/\",\n",
    "                  1\n",
    "              ),\n",
    "              \"yyyy/MM/dd\"\n",
    "          )\n",
    "      )\n",
    ")\n",
    "\n",
    "# Vykresli, ať vidíš, že se ti načetly všechny dny\n",
    "df_enriched.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9fef24e-1219-4f1d-8ad6-1eefc5fad245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Přepiš (overwrite) tabulku customers\n",
    "df_enriched.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(TARGET_TABLE)\n",
    "\n",
    "print(f\"✅ Data written to {TARGET_TABLE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "456df0c1-7fdd-464d-8cde-0116032f0bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select distinct snapshot_date from dev_bronze.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859b2095-5d17-4d0f-a1c6-e8c5cb6524d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook buňka 1: Načtení a obohacení všech tabulek\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, regexp_extract, to_date\n",
    "\n",
    "# --- Konfigurace ---\n",
    "RAW_PATH = \"/Volumes/principal_lab_db/landing/operational_data\"\n",
    "CATALOG  = \"principal_lab_db\"\n",
    "SCHEMA   = \"dev_bronze\"\n",
    "\n",
    "# Specifikace tabulek: (jméno, pattern, snapshot?)\n",
    "spec = [\n",
    "    (\"agents\",    \"agents/*/*/*/*.csv\",   True),    # 3 úrovně: YYYY/MM/DD\n",
    "    (\"customers\",\"customers/*/*/*/*.csv\", True),\n",
    "    (\"policies\",  \"policies/*/*/*/*.csv\",  True),\n",
    "    (\"claims\",    \"claims/*.csv\",           False),\n",
    "    (\"products\",  \"products/*.csv\",         False),\n",
    "]\n",
    "\n",
    "# Funkce pro obohacení\n",
    "def enrich(df, snapshot: bool):\n",
    "    df2 = (\n",
    "        df\n",
    "        .withColumn(\"ingestion_ts\", current_timestamp())\n",
    "        .withColumn(\"source_file\", input_file_name())\n",
    "    )\n",
    "    if snapshot:\n",
    "        df2 = df2.withColumn(\n",
    "            \"snapshot_date\",\n",
    "            to_date(\n",
    "                regexp_extract(\n",
    "                    input_file_name(),\n",
    "                    r\"/(\\d{4}/\\d{2}/\\d{2})/\", 1\n",
    "                ),\n",
    "                \"yyyy/MM/dd\"\n",
    "            )\n",
    "        )\n",
    "    return df2\n",
    "\n",
    "# Načteme a obohatíme všechny\n",
    "dfs = {}\n",
    "for table, pattern, is_snap in spec:\n",
    "    path = f\"{RAW_PATH}/{pattern}\"\n",
    "    print(f\"Ingesting {table} from {path}\")\n",
    "    df = spark.read.option(\"header\", True).csv(path)\n",
    "    df_enriched = enrich(df, is_snap)\n",
    "    dfs[table] = df_enriched  # uložíme si do dictu\n",
    "    df_enriched.show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469602e8-609a-4d7f-b18e-ba53fef118ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table, _, _ in spec:\n",
    "    TARGET = f\"{CATALOG}.{SCHEMA}.{table}\"\n",
    "    print(f\"Writing {table} → {TARGET}\")\n",
    "    dfs[table] \\\n",
    "      .write \\\n",
    "      .format(\"delta\") \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .option(\"overwriteSchema\", \"true\") \\\n",
    "      .saveAsTable(TARGET)\n",
    "    print(f\"✅ {table} written\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5637402806099337,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "test_dlt_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
