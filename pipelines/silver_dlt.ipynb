{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "401e2381-903f-48cf-ad33-eaa3951f4465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import json\n",
    "from pyspark.sql.functions import col, regexp_replace, from_json, udf, lower, initcap, upper, split, lit, concat, trim, when, isnull, expr, get, size, concat_ws, slice\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType, BooleanType, IntegerType, DateType\n",
    "\n",
    "# ——————————————————————————————\n",
    "#  COMMON SETUP\n",
    "# ——————————————————————————————\n",
    "env           = spark.conf.get(\"pipeline.env\") # prod dev prostredi\n",
    "catalog       = \"principal_lab_db\"\n",
    "bronze_schema = f\"{env}_bronze\"\n",
    "silver_schema = f\"{env}_silver\"\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {silver_schema}\")\n",
    "\n",
    "# mapování SCD typů\n",
    "scd_type_map = {\n",
    "    \"SCD1\": 1, \"SCD2\": 2,\n",
    "    \"1\": 1,     \"2\": 2,\n",
    "    None: 0,     \"\": 0\n",
    "}\n",
    "\n",
    "# ——————————————————————————————\n",
    "#  COMMON FUNCTIONS\n",
    "# ——————————————————————————————\n",
    "def get_table_expectations(table_name:str,catalog:str,enviroment:str) -> dict:\n",
    "    \"\"\"\n",
    "    Return dlt expectations values for desired table.\n",
    "    Args:\n",
    "        table_name:str table name\n",
    "        catalog:str catalog where is stored expectation setup table\n",
    "        enviroment:str use in schema name for setup table\n",
    "    Returns:\n",
    "        :dict all expectations for table  \n",
    "    \"\"\"\n",
    "    df = spark.read.table(f\"{catalog}.config_{enviroment}.expectations_setup\").filter(f\"table_name == '{table_name}'\")\n",
    "    df = df.select(\"expectation_name\",\"expectation_value\")\n",
    "    return {row[\"expectation_name\"]: row[\"expectation_value\"] for row in df.collect()}\n",
    "\n",
    "# ---------------------------------------------policies-------------------------------------------------------------\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  1) NAČTENÍ METADATA PRO POLICIES\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "config = (\n",
    "    spark.table(f\"{catalog}.config_{env}.table_lookup\")\n",
    "         .filter(col(\"table_name\") == \"dim_policies\")\n",
    "         .select(\"keys\", \"scd_type\", \"description\")\n",
    "         .first()\n",
    ")\n",
    "keys_raw       = config[\"keys\"]\n",
    "scd_raw        = config[\"scd_type\"]\n",
    "description    = config[\"description\"]\n",
    "\n",
    "# rozparsování business-keys\n",
    "business_keys = json.loads(keys_raw) if isinstance(keys_raw, str) else keys_raw\n",
    "scd_type       = scd_type_map.get(str(scd_raw).upper(), 0)\n",
    "\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  2) PRAVIDLA PRO VYMAZÁNÍ ŠPATNÝCH ZÁZNAMŮ\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "expectation_rules = {\n",
    "    \"policies\": {\n",
    "        \"expectations\": [{\n",
    "            \"valid_policy_id\":   \"policy_id IS NOT NULL\",\n",
    "            \"valid_agent_id\":    \"agent_id  IS NOT NULL\",\n",
    "            \"valid_customer_id\": \"customer_id IS NOT NULL\",\n",
    "            \"valid_product_id\":  \"product_id IS NOT NULL\",\n",
    "            \"positive_premium\":  \"premium > 0\",\n",
    "            \"valid_snapshot\":    \"snapshot_date IS NOT NULL\"\n",
    "        }]\n",
    "    }\n",
    "}\n",
    "pol_rules = expectation_rules[\"policies\"][\"expectations\"][0]\n",
    "pol_expr  = \"NOT({0})\".format(\" AND \".join(pol_rules.values()))\n",
    "\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  3) VIEWČKO\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "@dlt.table(\n",
    "    name=\"policies_clean_quarantine_rules\",\n",
    "    comment=\"apply expectations for policies\",\n",
    "    partition_cols=[\"is_quarantined\"]\n",
    ")\n",
    "@dlt.expect_all(pol_rules)\n",
    "def policies_clean_quarantine():\n",
    "    bronze = spark.readStream.table(f\"{catalog}.{bronze_schema}.policies_bronze\")\n",
    "    return (\n",
    "        bronze\n",
    "        .withColumn(\"policy_id\",    trim(col(\"policy_id\")))\n",
    "        .withColumn(\"agent_id\",     trim(col(\"agent_id\")))\n",
    "        .withColumn(\"customer_id\",  trim(col(\"customer_id\")))\n",
    "        .withColumn(\"product_id\",   trim(col(\"product_id\")))\n",
    "        .withColumn(\"premium\",      col(\"premium\").cast(DoubleType()))\n",
    "        .withColumn(\"coverages\",    col(\"coverages\"))\n",
    "        .withColumn(\"snapshot_date\", col(\"snapshot_date\").cast(DateType()))\n",
    "        .withColumn(\"is_quarantined\", expr(pol_expr))\n",
    "        .drop(\"_rescued_data\", \"source_file\", \"ingestion_ts\")\n",
    "    )\n",
    "\n",
    "\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  4) ŠPATNÉ X DOBRÉ ZÁZNAMY\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "@dlt.table(\n",
    "    name=\"policies_clean_good_records\",\n",
    "    comment=\"policies good (passed expectations)\"\n",
    ")\n",
    "def policies_good():\n",
    "    return (\n",
    "        dlt.read_stream(\"policies_clean_quarantine_rules\")\n",
    "           .filter(\"is_quarantined = false\")\n",
    "           .drop(\"is_quarantined\")\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"policies_clean_bad_records\",\n",
    "    comment=\"policies bad (failed expectations)\"\n",
    ")\n",
    "def policies_bad():\n",
    "    return (\n",
    "        dlt.read_stream(\"policies_clean_quarantine_rules\")\n",
    "           .filter(\"is_quarantined = true\")\n",
    "           .drop(\"is_quarantined\")\n",
    "    )\n",
    "\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  5) SCD2 DIMENZE\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "if scd_type == 2:\n",
    "    dlt.create_streaming_table(\n",
    "        name=\"dim_policies\",\n",
    "        comment=description,\n",
    "        table_properties={\"quality\": \"silver\"}\n",
    "    )\n",
    "    dlt.apply_changes(\n",
    "        target=\"dim_policies\",\n",
    "        source=\"policies_clean_good_records\",\n",
    "        keys=business_keys,\n",
    "        sequence_by=col(\"snapshot_date\"),\n",
    "        ignore_null_updates=False,\n",
    "        stored_as_scd_type=\"2\",\n",
    "        track_history_except_column_list=[\"snapshot_date\"]\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Nepodporovaný SCD typ: {scd_raw}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------- customer -------------------------------------------------------------\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  1) NACTENI METADATA PRO CUSTOMERS\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "config_customers = spark.table(f\"{catalog}.config_{env}.table_lookup\") \\\n",
    "    .filter(col(\"table_name\") == \"dim_customers\") \\\n",
    "    .select(\"keys\", \"scd_type\", \"description\") \\\n",
    "    .first()\n",
    "\n",
    "keys_raw_customers = config_customers[\"keys\"]\n",
    "scd_type_raw_customers = config_customers[\"scd_type\"]\n",
    "description_customers = config_customers[\"description\"]\n",
    "\n",
    "business_keys_customer = json.loads(keys_raw_customers) if isinstance(keys_raw_customers, str) else keys_raw_customers\n",
    "scd_type_customers = scd_type_map.get(str(scd_type_raw_customers).upper(), 0)\n",
    "\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  2) DEFINICE FUNKCI PRO CISTENI DAT PRO CUSTOMERS\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "def clean_last_name(value:str) -> str:\n",
    "    \"\"\"\n",
    "    Remove school titles after last name and take last word in last name. May occur values like 'Jimmie Smith Phd'. \n",
    "    In this case we want just Smith.\n",
    "    Args:\n",
    "        value:str raw value of row in desired column\n",
    "    Return:\n",
    "        :str cleaned value of row in desired column\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    forbidden_values = ['md','phd','dds']\n",
    "    words = value.strip().split()\n",
    "    words = [w.lower().capitalize() for w in words if w.lower() not in forbidden_values]\n",
    "    return words[-1] if words else None\n",
    "\n",
    "\n",
    "def clean_first_name(value:str) -> str:\n",
    "    \"\"\"\n",
    "    Removes prefixes from first name like Mrs.,etc.. or if first name have only Mrs. or Mr. return NULL.\n",
    "    Args:\n",
    "        value:str raw value of row in desired column\n",
    "    Return:\n",
    "        :str cleaned value of row in desired column\n",
    "    \"\"\"\n",
    "\n",
    "    if value is None:\n",
    "        return None\n",
    "    forbidden_values = ['mr','mrs','mr.','mrs.'] #if we have new prefixes, add here\n",
    "    words = value.strip().split()\n",
    "    words = [w.lower().capitalize() for w in words if w.lower() not in forbidden_values]\n",
    "    return words[0] if words else None\n",
    "\n",
    "#registrace udf funkcni pro pouziti v PySpark API\n",
    "clean_last_name_udf = udf(clean_last_name, StringType())\n",
    "clean_first_name_udf = udf(clean_first_name, StringType())\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  3) TABULKA PRO URCENI DOBRE/SPATNE ZAZNAMY\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#ziskani expectations hodnot pro datovou kvalitu\n",
    "customer_expectations_rules = get_table_expectations(\"customers\",catalog,env)\n",
    "customer_expectations_expr = \"NOT({0})\".format(\" AND \".join(customer_expectations_rules.values()))\n",
    "\n",
    "#vtvoreni tabulky kde se aplikuje datova kvalita\n",
    "#to, jestli zaznam projde datovou kvalitou urcuje sloupec is_quarantined\n",
    "@dlt.table(\n",
    "    name=\"customers_clean_quarantine_rules\",\n",
    "    comment=\"apply expectations rules for customers\",\n",
    "    partition_cols =[\"is_quarantined\"]\n",
    ")\n",
    "@dlt.expect_all(customer_expectations_rules)\n",
    "def customer_data_clean_quarantine():\n",
    "    df_customer = dlt.readStream(f\"{catalog}.{bronze_schema}.customers_bronze\")\n",
    "    return (\n",
    "        df_customer\n",
    "        .withColumn(\"customer_id\",trim(\"customer_id\"))\n",
    "        .withColumn(\"first_name\", clean_first_name_udf(col(\"first_name\")))\n",
    "        .withColumn(\"last_name\",clean_last_name_udf(col(\"last_name\")))\n",
    "        .withColumn(\"email\", lower(trim(col(\"email\"))))\n",
    "        .withColumn(\"address_splt\",split(trim(col(\"address\")),','))\n",
    "        .withColumn(\"address\",concat(initcap(get(\"address_splt\",0)),lit(','),initcap(get(\"address_splt\",1)),lit(','),upper(get(\"address_splt\",2))))\n",
    "        .withColumn(\"income\",col(\"income\").cast(IntegerType()))\n",
    "        .withColumn(\"contact_methods_raw\", regexp_replace(col(\"preferences.contact_methods\"), r'\\\\\"', '\"'))\n",
    "        .withColumn(\"contact_methods\", from_json(col(\"contact_methods_raw\"), ArrayType(StringType()))) \n",
    "        .withColumn(\"preferred_language\", trim(col(\"preferences.preferred_language\"))) \n",
    "        .withColumn(\"newsletter_opt_in\", col(\"preferences.newsletter_opt_in\").cast(BooleanType())) \n",
    "        .withColumn(\"is_quarantined\",expr(customer_expectations_expr))\n",
    "        .drop(\"address_splt\",\"contact_methods_raw\",\"preferences\",\"_rescued_data\", \"source_file\", \"ingestion_ts\")\n",
    "    )\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  4) SPATNE X DOBRE ZAZNAMY\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#zapis dat do tabulky kde mame zaznamy ktere vyhovuji datove kvalite\n",
    "@dlt.table(name='customers_clean_good_records',comment='customers cleanded and validate data')\n",
    "def customer_clean():\n",
    "    df_customer = dlt.readStream('customers_clean_quarantine_rules')\n",
    "    return (\n",
    "        df_customer\n",
    "        .filter(\"is_quarantined=false\")\n",
    "        .drop(\"is_quarantined\")\n",
    "    )\n",
    "\n",
    "#zapis dat do tabulky kde mame zaznamy ktere nevyhovuji datove kvalite\n",
    "@dlt.table(name='customers_clean_bad_records',comment='customers cleaned and bad data')\n",
    "def customer_clean():\n",
    "    df_customer = dlt.readStream('customers_clean_quarantine_rules')\n",
    "    return (\n",
    "        df_customer\n",
    "        .filter(\"is_quarantined=true\")\n",
    "        .drop(\"is_quarantined\")\n",
    "    )\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  5) SCD2 DIMENZE \n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "if scd_type_customers == 2:\n",
    "    dlt.create_streaming_table(\n",
    "        name=\"dim_customers\",\n",
    "        comment=description_customers,\n",
    "        table_properties={\"quality\": \"silver\"}\n",
    "        )\n",
    "    dlt.apply_changes(\n",
    "        target = \"dim_customers\",\n",
    "        source = \"customers_clean_good_records\",\n",
    "        keys = business_keys_customer,\n",
    "        sequence_by = col(\"snapshot_date\"),\n",
    "        ignore_null_updates=False,\n",
    "        stored_as_scd_type=\"2\",\n",
    "        track_history_except_column_list=['snapshot_date']\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Nepodporovaný nebo chybějící SCD typ: {scd_type_raw_customers}\")\n",
    "\n",
    "# --------------------------------------------- agents -------------------------------------------------------------\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  1) NACTENI METADATA PRO AGENTS\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "config_agents = spark.table(f\"{catalog}.config_{env}.table_lookup\") \\\n",
    "    .filter(col(\"table_name\") == \"dim_agents\") \\\n",
    "    .select(\"keys\", \"scd_type\", \"description\") \\\n",
    "    .first()\n",
    "\n",
    "keys_raw_agents = config_agents[\"keys\"]\n",
    "scd_type_raw_agents = config_agents[\"scd_type\"]\n",
    "description_agents = config_agents[\"description\"]\n",
    "\n",
    "business_keys_agents = json.loads(keys_raw_agents) if isinstance(keys_raw_agents, str) else keys_raw_agents\n",
    "scd_type_agents = scd_type_map.get(str(scd_type_raw_agents).upper(), 0)\n",
    "\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  2) DEFINICE FUNKCI PRO CISTENI DAT PRO AGENTS\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "def clean_full_name(value:str) -> str:\n",
    "    \"\"\"\n",
    "    Remove prefixes/sufixes from names.\n",
    "    \"\"\"\n",
    "    forbidden_prefixes = ['mr.', 'mrs.', 'mr','mrs','dds','phd','md']\n",
    "    if value == None:\n",
    "        return None\n",
    "    name_spl = value.strip().lower().split(\" \")\n",
    "    cleaned_name = [word.capitalize() for word in name_spl if word not in forbidden_prefixes]\n",
    "    return ' '.join(cleaned_name)\n",
    "\n",
    "#registrace udf funkcni pro pouziti v PySpark API\n",
    "clean_full_name_udf = udf(clean_full_name, StringType())\n",
    "\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  3) TABULKA PRO URCENI DOBRE/SPATNE ZAZNAMY - DATA KVALITA\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#ziskani expectations hodnot pro datovou kvalitu\n",
    "agents_expectations_rules = get_table_expectations(\"agents\",catalog,env)\n",
    "agents_expectations_expr = \"NOT({0})\".format(\" AND \".join(agents_expectations_rules.values()))\n",
    "\n",
    "#vtvoreni tabulky kde se aplikuje datova kvalita\n",
    "#to, jestli zaznam projde datovou kvalitou urcuje sloupec is_quarantined\n",
    "@dlt.table(\n",
    "    name=\"agents_clean_quarantine_rules\",\n",
    "    comment=\"apply expectations rules for agents\",\n",
    "    partition_cols =[\"is_quarantined\"]\n",
    ")\n",
    "@dlt.expect_all(agents_expectations_rules)\n",
    "def agents_data_clean_quarantine():\n",
    "    df_agents = dlt.readStream(f\"{catalog}.{bronze_schema}.agents_bronze\")\n",
    "    return (\n",
    "        df_agents\n",
    "        .withColumn(\"agent_id\",trim(col(\"agent_id\")))\n",
    "        .withColumn(\"name\",clean_full_name_udf(col(\"name\")))\n",
    "        .withColumn(\"name_splt\",split(col(\"name\"),\" \"))\n",
    "        .withColumn(\"first_name\", when(isnull(col(\"name_splt\")),None).when(size(col(\"name_splt\"))==1,None).otherwise(concat_ws(\" \", slice(col(\"name_splt\"), 1, size(col(\"name_splt\")) - 1))))\n",
    "        .withColumn(\"last_name\",when(isnull(col(\"name_splt\")), None).when(size(col(\"name_splt\"))==1,None).otherwise(col(\"name_splt\")[size(col(\"name_splt\")) - 1]))\n",
    "        .withColumn(\"region\",initcap(trim(col(\"region\"))))\n",
    "        .withColumn(\"email\",lower(trim(col(\"email\"))))\n",
    "        .withColumn(\"start_date\", col(\"start_date\").cast(DateType()))\n",
    "        .withColumn(\"languages_raw\",regexp_replace(col(\"metadata.languages\"), r'\\\\\"', '\"'))\n",
    "        .withColumn(\"languages\", from_json(col(\"languages_raw\"),ArrayType(StringType())))\n",
    "        .withColumn(\"certifications_raw\",regexp_replace(col(\"metadata.certifications\"), r'\\\\\"', '\"'))\n",
    "        .withColumn(\"certifications\", from_json(col(\"certifications_raw\"),ArrayType(StringType())))\n",
    "        .withColumn(\"is_quarantined\",expr(agents_expectations_expr))\n",
    "        .drop(\"name\",\"metadata\",\"name_splt\",\"languages_raw\",\"certifications_raw\",\"_rescued_data\", \"source_file\", \"ingestion_ts\")\n",
    "    )\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  4) SPATNE X DOBRE ZAZNAMY\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#zapis dat do tabulky kde mame zaznamy ktere vyhovuji datove kvalite\n",
    "@dlt.table(name='agents_clean_good_records',comment='agents cleanded and validate data')\n",
    "def customer_clean():\n",
    "    df_customer = dlt.readStream('agents_clean_quarantine_rules')\n",
    "    return (\n",
    "        df_customer\n",
    "        .filter(\"is_quarantined=false\")\n",
    "        .drop(\"is_quarantined\")\n",
    "    )\n",
    "\n",
    "#zapis dat do tabulky kde mame zaznamy ktere nevyhovuji datove kvalite\n",
    "@dlt.table(name='agents_clean_bad_records',comment='agents cleaned and bad data')\n",
    "def customer_clean():\n",
    "    df_customer = dlt.readStream('agents_clean_quarantine_rules')\n",
    "    return (\n",
    "        df_customer\n",
    "        .filter(\"is_quarantined=true\")\n",
    "        .drop(\"is_quarantined\")\n",
    "    )\n",
    "\n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "#  5) SCD2 DIMENZE \n",
    "# —————————————————————————————————————————————————————————————————————————————————\n",
    "if scd_type_customers == 2:\n",
    "    dlt.create_streaming_table(\n",
    "        name=\"dim_agents\",\n",
    "        comment=description_agents,\n",
    "        table_properties={\"quality\": \"silver\"}\n",
    "        )\n",
    "    dlt.apply_changes(\n",
    "        target = \"dim_agents\",\n",
    "        source = \"agents_clean_good_records\",\n",
    "        keys = business_keys_agents,\n",
    "        sequence_by = col(\"snapshot_date\"),\n",
    "        ignore_null_updates=False,\n",
    "        stored_as_scd_type=\"2\",\n",
    "        track_history_except_column_list=['snapshot_date']\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Nepodporovaný nebo chybějící SCD typ: {scd_type_raw_customers}\")\n",
    "\n",
    "# ---------------------------------------------products-------------------------------------------------------------\n",
    "\n",
    "# Load the table configuration from the config\n",
    "config = spark.table(f\"{catalog}.config_{env}.table_lookup\") \\\n",
    "    .filter(col(\"table_name\") == \"dim_products\") \\\n",
    "    .select(\"scd_type\", \"description\") \\\n",
    "    .first()\n",
    "\n",
    "# Validate that the configuration was found\n",
    "if config is None:\n",
    "    raise ValueError(f\"Configuration for 'dim_products' not found in table {catalog}.config_{env}.table_lookup\")\n",
    "\n",
    "# Extract metadata values\n",
    "description = config[\"description\"]\n",
    "scd_type_raw = config[\"scd_type\"]\n",
    "\n",
    "# Map SCD type from string to an integer value\n",
    "scd_type_map = {\n",
    "    \"SCD1\": 1,\n",
    "    \"SCD2\": 2,\n",
    "    \"1\": 1,\n",
    "    \"2\": 2,\n",
    "    None: 0,\n",
    "    \"\": 0\n",
    "}\n",
    "scd_type = scd_type_map.get(str(scd_type_raw).upper(), 0)\n",
    "\n",
    "# Define a streaming view to clean the bronze table\n",
    "@dlt.view(name=\"products_bronze_clean\")\n",
    "def products_bronze_clean():\n",
    "    df = spark.readStream.table(f\"{catalog}.{bronze_schema}.products_bronze\")\n",
    "\n",
    "    cleaned_df = (\n",
    "        df\n",
    "        .filter(col(\"product_id\").isNotNull())  \n",
    "        .withColumn(\"product_id\", trim(col(\"product_id\").cast(StringType())))  \n",
    "        .withColumn(\"product_name\", initcap(trim(col(\"product_name\").cast(StringType()))))  \n",
    "        .withColumn(\"category\", initcap(trim(col(\"category\").cast(StringType()))))  \n",
    "        .dropDuplicates([\"product_id\", \"product_name\", \"category\"]) \n",
    "        .drop(\"_rescued_data\", \"source_file\", \"ingestion_ts\")  \n",
    "    )\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "# create Silver table if SCD type is 0 or 1 \n",
    "if scd_type in (0, 1): \n",
    "\n",
    "    @dlt.table(\n",
    "        name=\"dim_products\",\n",
    "        comment=description,\n",
    "        spark_conf={\"spark.databricks.delta.schema.autoMerge.enabled\": \"true\"},  # Enable schema evolution\n",
    "        table_properties={\n",
    "            \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "            \"delta.autoOptimize.autoCompact\": \"true\",\n",
    "            \"pipelines.autoOptimize.zOrderCols\": \"product_id\",  # Optimize queries on product_id\n",
    "            \"pipelines.reset.allowed\": \"true\"  # Allow pipeline reset to overwrite table\n",
    "        }\n",
    "    )\n",
    "    def dim_products():\n",
    "        return dlt.read_stream(\"products_bronze_clean\")  \n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"SCD type '{scd_type_raw}' is not supported for 'dim_products'\")\n",
    "\n",
    "# ---------------------------------------------FACT TABLES---------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------- claims---------------------------------------------------------\n",
    "\n",
    "# Load the table configuration from the config\n",
    "config = spark.table(f\"{catalog}.config_{env}.table_lookup\") \\\n",
    "    .filter(col(\"table_name\") == \"fact_claims\") \\\n",
    "    .select(\"scd_type\", \"description\") \\\n",
    "    .first()\n",
    "\n",
    "# Validate that the configuration was found\n",
    "if config is None:\n",
    "    raise ValueError(f\"Configuration for 'fact_claims' not found in table {catalog}.config_{env}.table_lookup\")\n",
    "\n",
    "# Extract metadata values\n",
    "description = config[\"description\"]\n",
    "scd_type_raw = config[\"scd_type\"]\n",
    "\n",
    "# Map SCD type from string to an integer value\n",
    "scd_type_map = {\n",
    "    \"SCD1\": 1,\n",
    "    \"SCD2\": 2,\n",
    "    \"1\": 1,\n",
    "    \"2\": 2,\n",
    "    None: 0,\n",
    "    \"\": 0\n",
    "}\n",
    "scd_type = scd_type_map.get(str(scd_type_raw).upper(), 0)\n",
    "\n",
    "# Define a streaming view to clean the bronze claims table\n",
    "@dlt.view(name=\"claims_bronze_clean\")\n",
    "def claims_bronze_clean():\n",
    "    df = spark.readStream.table(f\"{catalog}.{bronze_schema}.claims_bronze\")\n",
    "\n",
    "    cleaned_df = (\n",
    "        df\n",
    "        .filter(col(\"claim_id\").isNotNull()) \n",
    "        .withColumn(\"claim_id\", trim(col(\"claim_id\").cast(StringType()))) \n",
    "        .withColumn(\"policy_id\", trim(col(\"policy_id\").cast(StringType())))  \n",
    "        .withColumn(\"agent_id\", trim(col(\"agent_id\").cast(StringType())))  \n",
    "        .withColumn(\"claim_date\", col(\"claim_date\").cast(DateType())) \n",
    "        .withColumn(\"amount\", col(\"amount\").cast(IntegerType())) \n",
    "        .dropDuplicates([\"claim_id\"]) \n",
    "        .drop(\"_rescued_data\", \"source_file\", \"ingestion_ts\")  \n",
    "    )\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "# Create Silver table for fact table (SCD type 0)\n",
    "if scd_type == 0:\n",
    "    @dlt.table(\n",
    "        name=\"fact_claims\",\n",
    "        comment=description,\n",
    "        spark_conf={\"spark.databricks.delta.schema.autoMerge.enabled\": \"true\"},  # Enable schema evolution\n",
    "        table_properties={\n",
    "            \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "            \"delta.autoOptimize.autoCompact\": \"true\",\n",
    "            \"pipelines.autoOptimize.zOrderCols\": \"claim_id\",  # Optimize queries on claim_id\n",
    "            \"pipelines.reset.allowed\": \"true\"  # Allow pipeline reset to overwrite table\n",
    "        }\n",
    "    )\n",
    "    def fact_claims():\n",
    "        return dlt.read_stream(\"claims_bronze_clean\")\n",
    "\n",
    "    # Define data quality expectations\n",
    "    dlt.expect_or_fail(\"valid_claim_id\", \"claim_id IS NOT NULL\")\n",
    "    dlt.expect_or_fail(\"valid_policy_id\", \"policy_id IS NOT NULL\")\n",
    "    dlt.expect_or_fail(\"valid_agent_id\", \"agent_id IS NOT NULL\")\n",
    "    dlt.expect_or_fail(\"positive_amount\", \"amount > 0\")\n",
    "    dlt.expect_or_fail(\"valid_claim_date\", \"claim_date IS NOT NULL AND claim_date = CAST(CAST(claim_date AS STRING) AS DATE)\")\n",
    "else:\n",
    "    raise ValueError(f\"SCD type '{scd_type_raw}' is not supported for 'fact_claims'\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------- premium ---------------------------------------------------------\n",
    "\n",
    "# Load the table configuration from the config\n",
    "config = spark.table(f\"{catalog}.config_{env}.table_lookup\") \\\n",
    "    .filter(col(\"table_name\") == \"fact_premium_transactions\") \\\n",
    "    .select(\"scd_type\", \"description\") \\\n",
    "    .first()\n",
    "\n",
    "# Validate that the configuration was found\n",
    "if config is None:\n",
    "    raise ValueError(f\"Configuration for 'fact_premium_transactions' not found in table {catalog}.config_{env}.table_lookup\")\n",
    "\n",
    "# Extract metadata values\n",
    "description = config[\"description\"]\n",
    "scd_type_raw = config[\"scd_type\"]\n",
    "\n",
    "# Map SCD type from string to an integer value\n",
    "scd_type_map = {\n",
    "    \"SCD1\": 1,\n",
    "    \"SCD2\": 2,\n",
    "    \"1\": 1,\n",
    "    \"2\": 2,\n",
    "    None: 0,\n",
    "    \"\": 0\n",
    "}\n",
    "scd_type = scd_type_map.get(str(scd_type_raw).upper(), 0)\n",
    "\n",
    "# Define a streaming view to clean the bronze premium transactions table\n",
    "@dlt.view(name=\"premium_transactions_bronze_clean\")\n",
    "def premium_transactions_bronze_clean():\n",
    "    df = spark.readStream.table(f\"{catalog}.{bronze_schema}.premium_transactions_bronze\")\n",
    "\n",
    "    cleaned_df = (\n",
    "        df\n",
    "        .filter(col(\"premium_txn_id\").isNotNull())  \n",
    "        .withColumn(\"premium_txn_id\", trim(col(\"premium_txn_id\").cast(StringType())))  \n",
    "        .withColumn(\"policy_id\", trim(col(\"policy_id\").cast(StringType())))  \n",
    "        .withColumn(\"agent_id\", trim(col(\"agent_id\").cast(StringType())))  \n",
    "        .withColumn(\"due_date\", col(\"due_date\").cast(DateType())) \n",
    "        .withColumn(\"premium_amount\", col(\"premium_amount\").cast(DoubleType()))  \n",
    "        .withColumn(\"paid_flag\", col(\"paid_flag\").cast(BooleanType())) \n",
    "        .withColumn(\"payment_date\", col(\"payment_date\").cast(DateType())) \n",
    "        .withColumn(\"snapshot_date\", col(\"snapshot_date\").cast(DateType()))\n",
    "        .dropDuplicates([\"premium_txn_id\"])  \n",
    "        .drop(\"_rescued_data\", \"source_file\", \"ingestion_ts\")  \n",
    "    )\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "# Create Silver table for fact_premium_transactions table (SCD type 0)\n",
    "if scd_type == 0:\n",
    "    @dlt.table(\n",
    "        name=\"fact_premium_transactions\",\n",
    "        comment=description,\n",
    "        spark_conf={\"spark.sql.databricks.delta.schema.autoMerge.enabled\": \"true\"}, # Enable schema evolution\n",
    "        table_properties={\n",
    "            \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "            \"delta.autoOptimize.autoCompact\": \"true\",\n",
    "            \"pipelines.autoOptimize.zOrderCols\": \"premium_txn_id\",  # Optimize queries on premium_txn_id\n",
    "            \"pipelines.reset.allowed\": \"true\"  # Allow pipeline reset to overwrite table\n",
    "        }\n",
    "    )\n",
    "    def fact_premium_transactions():\n",
    "        return dlt.read_stream(\"premium_transactions_bronze_clean\")\n",
    "\n",
    "    # Define data quality expectations\n",
    "    dlt.expect_or_fail(\"valid_premium_txn_id\", \"premium_txn_id IS NOT NULL\")\n",
    "    dlt.expect_or_fail(\"valid_policy_id\", \"policy_id IS NOT NULL\")\n",
    "    dlt.expect_or_fail(\"valid_agent_id\", \"agent_id IS NOT NULL\")\n",
    "    dlt.expect_or_fail(\"positive_premium_amount\", \"premium_amount > 0\")\n",
    "    dlt.expect_or_fail(\"valid_due_date\", \"due_date IS NOT NULL AND due_date = CAST(CAST(due_date AS STRING) AS DATE)\")\n",
    "    dlt.expect_or_fail(\"valid_paid_flag\", \"paid_flag IS NOT NULL\")\n",
    "    dlt.expect(\"valid_payment_date\", \"paid_flag = FALSE OR (paid_flag = TRUE AND payment_date IS NOT NULL AND payment_date = CAST(CAST(payment_date AS STRING) AS DATE))\")\n",
    "    dlt.expect_or_fail(\"valid_snap_date\", \"snapshot_date IS NOT NULL AND snapshot_date = CAST(CAST(snapshot_date AS STRING) AS DATE)\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"SCD type '{scd_type_raw}' is not supported for 'fact_premium_transactions'\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
