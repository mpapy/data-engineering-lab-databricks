{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "401e2381-903f-48cf-ad33-eaa3951f4465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import json\n",
    "from pyspark.sql.functions import col, regexp_replace, from_json, udf, col, lower, initcap, upper, split, lit, concat, trim, when, isnull, expr, get, size, concat_ws,slice\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType, BooleanType,IntegerType,DateType\n",
    "\n",
    "# Získání prostředí (např. \"dev\" nebo \"prod\")\n",
    "env = spark.conf.get(\"pipeline.env\")\n",
    "catalog = \"principal_lab_db\"\n",
    "silver_schema = f\"{env}_silver\"\n",
    "bronze_schema = f\"{env}_bronze\"\n",
    "\n",
    "# Nastavení katalogu\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {silver_schema}\")\n",
    "\n",
    "# Načtení metadata z lookup tabulky\n",
    "config = spark.table(f\"{catalog}.config_{env}.table_lookup\") \\\n",
    "    .filter(col(\"table_name\") == \"dim_policies\") \\\n",
    "    .select(\"keys\", \"scd_type\", \"description\") \\\n",
    "    .first()\n",
    "\n",
    "# Parsování hodnot\n",
    "keys_raw = config[\"keys\"]\n",
    "scd_type_raw = config[\"scd_type\"]\n",
    "description = config[\"description\"]\n",
    "\n",
    "# Pokud je 'keys' uložen jako JSON řetězec, tak ho rozparsuj\n",
    "if isinstance(keys_raw, str):\n",
    "    business_keys = json.loads(keys_raw)\n",
    "else:\n",
    "    business_keys = keys_raw\n",
    "\n",
    "# Mapa pro SCD typ\n",
    "scd_type_map = {\n",
    "    \"SCD1\": 1,\n",
    "    \"SCD2\": 2,\n",
    "    \"1\": 1,\n",
    "    \"2\": 2,\n",
    "    None: 0,\n",
    "    \"\": 0\n",
    "}\n",
    "scd_type = scd_type_map.get(str(scd_type_raw).upper(), 0)\n",
    "\n",
    "# Debug pro sebe\n",
    "print(f\"Prostředí: {env}\")\n",
    "print(f\"Business Keys: {business_keys}\")\n",
    "print(f\"SCD Type: {scd_type}\")\n",
    "\n",
    "# Vytvoření view z Bronze vrstvy a čištění\n",
    "@dlt.view(name=\"policies_bronze_clean\")\n",
    "def policies_bronze_clean():\n",
    "    bronze_df = spark.readStream.table(f\"{catalog}.{bronze_schema}.policies_bronze\")\n",
    "\n",
    "    cleaned_df = bronze_df \\\n",
    "        .withColumn(\"premium\", col(\"premium\").cast(DoubleType())) \\\n",
    "        .drop(\"_rescued_data\", \"source_file\", \"ingestion_ts\") \n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "\n",
    "# Silver tabulka se SCD2 historií\n",
    "if scd_type == 2:\n",
    "    # Tabulka dim_policies ve stříbrné vrstvě (SCD2)\n",
    "    dlt.create_streaming_table(\n",
    "        name=\"dim_policies\",\n",
    "        comment=description,\n",
    "        table_properties={\"quality\": \"silver\"}\n",
    "    )\n",
    "\n",
    "    #  tady se vytváří historie přes __START_AT a __END_AT\n",
    "    dlt.apply_changes(\n",
    "        target=\"dim_policies\",\n",
    "        source=\"policies_bronze_clean\",\n",
    "        keys=business_keys,\n",
    "        sequence_by=col(\"snapshot_date\"),\n",
    "        ignore_null_updates=False,\n",
    "        stored_as_scd_type=\"2\",\n",
    "        track_history_except_column_list=[\"snapshot_date\"]\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Nepodporovaný nebo chybějící SCD typ: {scd_type_raw}\")\n",
    "\n",
    "#---- expectations rules for customers and agents table ------#\n",
    "expectation_rules = {\n",
    "    \"customer\": {\n",
    "        \"expectations\": [{\n",
    "                \"valid_last_name\": \"last_name IS NOT NULL\",\n",
    "                \"valid_income\":\"income > 0\"\n",
    "            }]\n",
    "        },\n",
    "    \"agents\":{\n",
    "        \"expectations\":[{  \n",
    "            \"valid_last_name\": \"last_name IS NOT NULL\",\n",
    "           }]\n",
    "        }\n",
    "}\n",
    "#------------------------------- customers table start  -------#\n",
    "config_customers = spark.table(f\"{catalog}.config_{env}.table_lookup\") \\\n",
    "    .filter(col(\"table_name\") == \"dim_customers\") \\\n",
    "    .select(\"keys\", \"scd_type\", \"description\") \\\n",
    "    .first()\n",
    "\n",
    "keys_raw_customers = config_customers[\"keys\"]\n",
    "scd_type_raw_customers = config_customers[\"scd_type\"]\n",
    "description_customers = config_customers[\"description\"]\n",
    "\n",
    "if isinstance(keys_raw_customers, str):\n",
    "    business_keys_customer = json.loads(keys_raw_customers)\n",
    "else:\n",
    "    business_keys_customer = keys_raw_customers\n",
    "\n",
    "scd_type_customers = scd_type_map.get(str(scd_type_raw_customers).upper(), 0)\n",
    "\n",
    "customer_expectations_expr = \"NOT({0})\".format(\" AND \".join(expectation_rules[\"customer\"][\"expectations\"][0].values()))\n",
    "\n",
    "def clean_last_name(value:str) -> str:\n",
    "    \"\"\"\n",
    "    Remove school titles after last name and take last word in last name. May occur values like 'Jimmie Smith Phd'. \n",
    "    In this case we want just Smith.\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    forbidden_values = ['md','phd','dds']\n",
    "    words = value.strip().split()\n",
    "    words = [w.lower().capitalize() for w in words if w.lower() not in forbidden_values]\n",
    "    return words[-1] if words else None\n",
    "\n",
    "\n",
    "def clean_first_name(value:str) -> str:\n",
    "    \"\"\"\n",
    "    Removes prefixes from first name like Mrs.,etc.. or if first name have only Mrs. or Mr. return NULL.\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    forbidden_values = ['mr','mrs','mr.','mrs.'] #if we have new prefixes, add here\n",
    "    words = value.strip().split()\n",
    "    words = [w.lower().capitalize() for w in words if w.lower() not in forbidden_values]\n",
    "    return words[0] if words else None\n",
    "\n",
    "clean_last_name_udf = udf(clean_last_name, StringType())\n",
    "clean_first_name_udf = udf(clean_first_name, StringType())\n",
    "\n",
    "#--- table for apply expectations ---#\n",
    "@dlt.table(\n",
    "    name=\"customers_clean_quarantine_rules\",\n",
    "    comment=\"apply expectations rules for customers\",\n",
    "    partition_cols =[\"is_quarantined\"]\n",
    ")\n",
    "@dlt.expect_all(expectation_rules[\"customer\"][\"expectations\"][0])\n",
    "def customer_data_clean_quarantine():\n",
    "    df_customer = dlt.readStream(f\"{catalog}.{bronze_schema}.customers_bronze\")\n",
    "    return (\n",
    "        df_customer\n",
    "        .withColumn(\"customer_id\",trim(\"customer_id\"))\n",
    "        .withColumn(\"first_name\", clean_first_name_udf(col(\"first_name\")))\n",
    "        .withColumn(\"last_name\",clean_last_name_udf(col(\"last_name\")))\n",
    "        .withColumn(\"email\", lower(trim(col(\"email\"))))\n",
    "        .withColumn(\"address_splt\",split(trim(col(\"address\")),','))\n",
    "        .withColumn(\"address\",concat(initcap(get(\"address_splt\",0)),lit(','),initcap(get(\"address_splt\",1)),lit(','),upper(get(\"address_splt\",2))))\n",
    "        .withColumn(\"income\",col(\"income\").cast(IntegerType()))\n",
    "        .withColumn(\"contact_methods_raw\", regexp_replace(col(\"preferences.contact_methods\"), r'\\\\\"', '\"'))\n",
    "        .withColumn(\"contact_methods\", from_json(col(\"contact_methods_raw\"), ArrayType(StringType()))) \n",
    "        .withColumn(\"preferred_language\", trim(col(\"preferences.preferred_language\"))) \n",
    "        .withColumn(\"newsletter_opt_in\", col(\"preferences.newsletter_opt_in\").cast(BooleanType())) \n",
    "        .withColumn(\"is_quarantined\",expr(customer_expectations_expr))\n",
    "        .drop(\"address_splt\",\"contact_methods_raw\",\"preferences\",\"_rescued_data\", \"source_file\", \"ingestion_ts\")\n",
    "    )\n",
    "\n",
    "#table where is stored customers data which is good\n",
    "@dlt.table(name='customers_clean_good_records',comment='customers cleanded and validate data')\n",
    "def customer_clean():\n",
    "    df_customer = dlt.readStream('customers_clean_quarantine_rules')\n",
    "    return (\n",
    "        df_customer\n",
    "        .filter(\"is_quarantined=false\")\n",
    "        .drop(\"is_quarantined\")\n",
    "    )\n",
    "\n",
    "#table where is stored customers data which is bad\n",
    "@dlt.table(name='customers_clean_bad_records',comment='customers cleaned and bad data')\n",
    "def customer_clean():\n",
    "    df_customer = dlt.readStream('customers_clean_quarantine_rules')\n",
    "    return (\n",
    "        df_customer\n",
    "        .filter(\"is_quarantined=true\")\n",
    "        .drop(\"is_quarantined\")\n",
    "    )\n",
    "\n",
    "#scd 2 table for customers\n",
    "if scd_type_customers == 2:\n",
    "    dlt.create_streaming_table(\n",
    "        name=\"dim_customers\",\n",
    "        comment=description_customers,\n",
    "        table_properties={\"quality\": \"silver\"}\n",
    "        )\n",
    "    dlt.apply_changes(\n",
    "        target = \"dim_customers\",\n",
    "        source = \"customers_clean_good_records\",\n",
    "        keys = business_keys_customer,\n",
    "        sequence_by = col(\"snapshot_date\"),\n",
    "        ignore_null_updates=False,\n",
    "        stored_as_scd_type=\"2\",\n",
    "        track_history_except_column_list=['snapshot_date']\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Nepodporovaný nebo chybějící SCD typ: {scd_type_raw_customers}\")\n",
    "\n",
    "#------------------------------- agents table start  -------#\n",
    "config_agents = spark.table(f\"{catalog}.config_{env}.table_lookup\") \\\n",
    "    .filter(col(\"table_name\") == \"dim_agents\") \\\n",
    "    .select(\"keys\", \"scd_type\", \"description\") \\\n",
    "    .first()\n",
    "\n",
    "keys_raw_agents = config_agents[\"keys\"]\n",
    "scd_type_raw_agents = config_agents[\"scd_type\"]\n",
    "description_agents = config_agents[\"description\"]\n",
    "\n",
    "if isinstance(keys_raw_agents, str):\n",
    "    business_keys_agents = json.loads(keys_raw_agents)\n",
    "else:\n",
    "    business_keys_agents = keys_raw_agents\n",
    "\n",
    "scd_type_agents = scd_type_map.get(str(scd_type_raw_agents).upper(), 0)\n",
    "\n",
    "agents_expectations_expr = \"NOT({0})\".format(\" AND \".join(expectation_rules[\"agents\"][\"expectations\"][0].values()))\n",
    "\n",
    "def clean_full_name(value:str) -> str:\n",
    "    \"\"\"\n",
    "    Remove prefixes/sufixes from names.\n",
    "    \"\"\"\n",
    "    forbidden_prefixes = ['mr.', 'mrs.', 'mr','mrs','dds','phd','md']\n",
    "    if value == None:\n",
    "        return None\n",
    "    name_spl = value.strip().lower().split(\" \")\n",
    "    cleaned_name = [word.capitalize() for word in name_spl if word not in forbidden_prefixes]\n",
    "    return ' '.join(cleaned_name)\n",
    "\n",
    "clean_full_name_udf = udf(clean_full_name, StringType())\n",
    "\n",
    "#--- agents table for apply expectations ---#\n",
    "@dlt.table(\n",
    "    name=\"agents_clean_quarantine_rules\",\n",
    "    comment=\"apply expectations rules for agents\",\n",
    "    partition_cols =[\"is_quarantined\"]\n",
    ")\n",
    "@dlt.expect_all(expectation_rules[\"agents\"][\"expectations\"][0])\n",
    "def agents_data_clean_quarantine():\n",
    "    df_agents = dlt.readStream(f\"{catalog}.{bronze_schema}.agents_bronze\")\n",
    "    return (\n",
    "        df_agents\n",
    "        .withColumn(\"agent_id\",trim(col(\"agent_id\")))\n",
    "        .withColumn(\"name\",clean_full_name_udf(col(\"name\")))\n",
    "        .withColumn(\"name_splt\",split(col(\"name\"),\" \"))\n",
    "        .withColumn(\"first_name\", when(isnull(col(\"name_splt\")),None).when(size(col(\"name_splt\"))==1,None).otherwise(concat_ws(\" \", slice(col(\"name_splt\"), 1, size(col(\"name_splt\")) - 1))))\n",
    "        .withColumn(\"last_name\",when(isnull(col(\"name_splt\")), None).when(size(col(\"name_splt\"))==1,None).otherwise(col(\"name_splt\")[size(col(\"name_splt\")) - 1]))\n",
    "        .withColumn(\"region\",initcap(trim(col(\"region\"))))\n",
    "        .withColumn(\"email\",lower(trim(col(\"email\"))))\n",
    "        .withColumn(\"start_date\", col(\"start_date\").cast(DateType()))\n",
    "        .withColumn(\"languages_raw\",regexp_replace(col(\"metadata.languages\"), r'\\\\\"', '\"'))\n",
    "        .withColumn(\"languages\", from_json(col(\"languages_raw\"),ArrayType(StringType())))\n",
    "        .withColumn(\"certifications_raw\",regexp_replace(col(\"metadata.certifications\"), r'\\\\\"', '\"'))\n",
    "        .withColumn(\"certifications\", from_json(col(\"certifications_raw\"),ArrayType(StringType())))\n",
    "        .withColumn(\"is_quarantined\",expr(agents_expectations_expr))\n",
    "        .drop(\"name\",\"metadata\",\"name_splt\",\"languages_raw\",\"certifications_raw\",\"_rescued_data\", \"source_file\", \"ingestion_ts\")\n",
    "    )\n",
    "\n",
    "#table where is stored agents data which is good\n",
    "@dlt.table(name='agents_clean_good_records',comment='agents cleanded and validate data')\n",
    "def customer_clean():\n",
    "    df_customer = dlt.readStream('agents_clean_quarantine_rules')\n",
    "    return (\n",
    "        df_customer\n",
    "        .filter(\"is_quarantined=false\")\n",
    "        .drop(\"is_quarantined\")\n",
    "    )\n",
    "\n",
    "#table where is stored agents data which is bad\n",
    "@dlt.table(name='agents_clean_bad_records',comment='agents cleaned and bad data')\n",
    "def customer_clean():\n",
    "    df_customer = dlt.readStream('agents_clean_quarantine_rules')\n",
    "    return (\n",
    "        df_customer\n",
    "        .filter(\"is_quarantined=true\")\n",
    "        .drop(\"is_quarantined\")\n",
    "    )\n",
    "\n",
    "#scd 2 table for agents\n",
    "if scd_type_customers == 2:\n",
    "    dlt.create_streaming_table(\n",
    "        name=\"dim_agents\",\n",
    "        comment=description_agents,\n",
    "        table_properties={\"quality\": \"silver\"}\n",
    "        )\n",
    "    dlt.apply_changes(\n",
    "        target = \"dim_agents\",\n",
    "        source = \"agents_clean_good_records\",\n",
    "        keys = business_keys_agents,\n",
    "        sequence_by = col(\"snapshot_date\"),\n",
    "        ignore_null_updates=False,\n",
    "        stored_as_scd_type=\"2\",\n",
    "        track_history_except_column_list=['snapshot_date']\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Nepodporovaný nebo chybějící SCD typ: {scd_type_raw_customers}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
