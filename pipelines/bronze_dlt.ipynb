{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed990190-c2b2-432c-a8ec-86db48c38817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ensure idempotent runs of your DLT pipeline in DEV:\n",
    "\n",
    "1. **Development Mode**: In the Databricks UI toggle your pipeline to Development Mode. This will drop and re-create managed tables on every pipeline update.\n",
    "\n",
    "2. **Alternate Schema**: Use separate `dev_bronze` and `prod_bronze` schemas to avoid conflicts.\n",
    "\n",
    "3. **Manual Cleanup (Once)**: If needed, run the following SQL commands just one time to clear stale managed tables:\n",
    "   DROP TABLE IF EXISTS principal_lab_db.dev_bronze.agents;\n",
    "   DROP TABLE IF EXISTS principal_lab_db.dev_bronze.customers;\n",
    "   DROP TABLE IF EXISTS principal_lab_db.dev_bronze.policies;\n",
    "   DROP TABLE IF EXISTS principal_lab_db.dev_bronze.claims;\n",
    "   DROP TABLE IF EXISTS principal_lab_db.dev_bronze.products;\n",
    "\"\"\"\n",
    "import dlt\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, BooleanType, DateType\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "# Dostanu prostředí - DEV nebo PROD (Prod nepoužívat)\n",
    "env = spark.conf.get(\"pipeline.env\")\n",
    "catalog = \"principal_lab_db\"\n",
    "bronze_schema = f\"{env}_bronze\"\n",
    "silver_schema = f\"{env}_silver\"\n",
    "\n",
    "# (Optionally read lookup table for config – here we manually specify the config for clarity)\n",
    "keys_map = {\n",
    "    \"customers\": [\"customer_id\"],   # Primární klíč\n",
    "    \"agents\":    [\"agent_id\"],\n",
    "    \"products\":  [\"product_id\"],\n",
    "    \"policies\":  [\"policy_id\"],\n",
    "    \"claims\":    [\"claim_id\"]\n",
    "}\n",
    "scd_type_map = {\n",
    "    \"customers\": \"SCD2\",   # dimension table with Type 2 changes\n",
    "    \"agents\":    \"SCD1\",   # dimension table with Type 1 changes\n",
    "    \"products\":  \"SCD1\",   # dimension (assume Type 1)\n",
    "    \"policies\":  None,     # fact table (no SCD)\n",
    "    \"claims\":    None      # fact table (no SCD)\n",
    "}\n",
    "\n",
    "# SCD2 Dimension Example: Customers (Dimension with historical tracking)\n",
    "@dlt.view(name=\"customers_cleaned\", comment=\"Streaming cleaned view of Customers data\")\n",
    "def customers_cleaned():\n",
    "    # Read bronze as a stream\n",
    "    return spark.readStream.table(f\"{catalog}.{bronze_schema}.customers_bronze\")  # :contentReference[oaicite:8]{index=8}\n",
    "\n",
    "# Create empty target table for SCD2 output (customers_scd2), with schema managed by DLT\n",
    "dlt.create_streaming_table(\n",
    "    name=\"customers_scd2\",\n",
    "    comment=\"Silver SCD2 dimension table for Customers (historical changes)\",\n",
    "    table_properties={\"quality\": \"silver\"}  # example property, Unity Catalog will store in silver schema\n",
    ")\n",
    "# Apply CDC flow to capture Type 2 changes into customers_scd2\n",
    "dlt.create_auto_cdc_flow(\n",
    "    target=\"customers_scd2\",\n",
    "    source=\"customers_cleaned\",\n",
    "    keys=keys_map[\"customers\"],            # e.g. [\"customer_id\"]\n",
    "    sequence_by=\"snapshot_date\",           # use snapshot_date as the sequence for ordering changes\n",
    "    stored_as_scd_type=2                   # SCD Type 2 (keeps history)\n",
    ")\n",
    " \n",
    "# SCD1 Dimension Example: Agents (Dimension with no history, latest state only)\n",
    "@dlt.view(name=\"agents_cleaned\", comment=\"Streaming cleaned view of Agents data\")\n",
    "def agents_cleaned():\n",
    "    return spark.readStream.table(f\"{catalog}.{bronze_schema}.agents_bronze\")\n",
    "\n",
    "@dlt.table(name=\"agents_silver\", comment=\"Silver table for latest Agents (SCD1)\")\n",
    "def agents_silver():\n",
    "    df = dlt.read(\"agents_cleaned\")  # read the cleaned stream (as a DataFrame)\n",
    "    # For each agent_id, keep only the record with max snapshot_date (latest snapshot)\n",
    "    window = Window.partitionBy(\"agent_id\").orderBy(col(\"snapshot_date\").desc())\n",
    "    latest_df = df.withColumn(\"rn\", row_number().over(window)) \\\n",
    "                  .filter(col(\"rn\") == 1) \\\n",
    "                  .drop(\"rn\")\n",
    "    return latest_df\n",
    "\n",
    "# SCD1 Dimension Example: Products (assume Products behave as Type 1 dimension)\n",
    "@dlt.view(name=\"products_cleaned\", comment=\"Streaming cleaned view of Products data\")\n",
    "def products_cleaned():\n",
    "    return spark.readStream.table(f\"{catalog}.{bronze_schema}.products_bronze\")\n",
    "\n",
    "@dlt.table(name=\"products_silver\", comment=\"Silver table for latest Products (SCD1)\")\n",
    "def products_silver():\n",
    "    df = dlt.read(\"products_cleaned\")\n",
    "    window = Window.partitionBy(\"product_id\").orderBy(col(\"snapshot_date\").desc())\n",
    "    latest_df = df.withColumn(\"rn\", row_number().over(window)) \\\n",
    "                  .filter(col(\"rn\") == 1) \\\n",
    "                  .drop(\"rn\")\n",
    "    return latest_df\n",
    "\n",
    "# Fact Table Example: Policies (Fact data with no SCD – append only)\n",
    "@dlt.table(name=\"policies_silver\", comment=\"Silver table for Policies fact data\")\n",
    "def policies_silver():\n",
    "    return spark.readStream.table(f\"{catalog}.{bronze_schema}.policies_bronze\")\n",
    "\n",
    "# Fact Table Example: Claims (Fact data with no SCD – append only)\n",
    "@dlt.table(name=\"claims_silver\", comment=\"Silver table for Claims fact data\")\n",
    "def claims_silver():\n",
    "    return spark.readStream.table(f\"{catalog}.{bronze_schema}.claims_bronze\")\n",
    "\n",
    "@dlt.table(name=\"premium_transactions_bronze\", comment=\"Bronze table for raw Premium Transactions data\")\n",
    "def premium_transactions_bronze():\n",
    "    path = f\"/Volumes/principal_lab_db/landing/operational_data/premium\"\n",
    "    return (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(path)\n",
    "            .select(\"*\", col(\"_metadata.file_path\").alias(\"source_file\"))\n",
    "            .withColumn(\"ingestion_ts\", current_timestamp())\n",
    "            .withColumn(\"snapshot_date\",\n",
    "                        to_date(regexp_extract(col(\"source_file\"),\n",
    "                                               r'/operational_data/[^/]+/(\\d{4}/\\d{2}/\\d{2})/', 1),\n",
    "                                \"yyyy/MM/dd\")))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
